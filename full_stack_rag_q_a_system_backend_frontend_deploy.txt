# 📦 Project: rag-stack

A full-stack Retrieval-Augmented Generation (RAG) app that lets users upload PDFs/Markdown/HTML/URLs, ask questions, and get grounded answers with citations.

This repo contains:
- `backend/` — FastAPI service for ingestion, retrieval, and answer generation (Chroma or FAISS; OpenAI/Sentence-Transformers/Cohere embeddings; OpenAI/Ollama LLMs)
- `frontend/` — React + Vite + Tailwind chat UI with file upload & cited answers
- `docker-compose.yml` — one command to run everything

---

## 🔧 1) Backend

**Structure**
```
backend/
  ├─ rag/
  │  ├─ __init__.py
  │  ├─ chunker.py
  │  ├─ ingest.py
  │  ├─ retriever.py
  │  ├─ llm.py
  │  ├─ utils.py
  │  └─ settings.py
  ├─ models.py
  ├─ main.py
  ├─ requirements.txt
  ├─ .env.example
  └─ README.md
```

### `backend/requirements.txt`
```
fastapi==0.112.2
uvicorn[standard]==0.30.6
python-multipart==0.0.9
pydantic==2.8.2
pydantic-settings==2.4.0
# Parsing
PyMuPDF==1.24.11
pdfminer.six==20231228
beautifulsoup4==4.12.3
readability-lxml==0.8.1
html2text==2024.2.26
markdown-it-py==3.0.0
# Embeddings & Vector Stores
chromadb==0.5.11
faiss-cpu==1.8.0.post1
sentence-transformers==3.0.1
numpy==1.26.4
# LLM clients
openai==1.51.0
cohere==5.9.4
# Utils
python-dotenv==1.0.1
requests==2.32.3
```

### `backend/.env.example`
```
# Choose one or more providers
OPENAI_API_KEY=
COHERE_API_KEY=

# Embeddings
EMBEDDINGS_PROVIDER=openai    # openai | sentence-transformers | cohere
EMBEDDINGS_MODEL=text-embedding-3-small  # or e.g. all-MiniLM-L6-v2 for sentence-transformers

# LLM provider/model
LLM_PROVIDER=openai           # openai | ollama
LLM_MODEL=gpt-4o-mini         # or e.g. llama3:8b (for ollama)

# Vector DB
VECTOR_DB=chroma              # chroma | faiss
PERSIST_DIR=./storage/chroma

# Server
HOST=0.0.0.0
PORT=8000
```

### `backend/rag/settings.py`
```python
from pydantic_settings import BaseSettings
from typing import Literal

class Settings(BaseSettings):
    openai_api_key: str | None = None
    cohere_api_key: str | None = None

    embeddings_provider: Literal["openai", "sentence-transformers", "cohere"] = "openai"
    embeddings_model: str = "text-embedding-3-small"

    llm_provider: Literal["openai", "ollama"] = "openai"
    llm_model: str = "gpt-4o-mini"

    vector_db: Literal["chroma", "faiss"] = "chroma"
    persist_dir: str = "./storage/chroma"

    host: str = "0.0.0.0"
    port: int = 8000

    class Config:
        env_file = ".env"
        case_sensitive = False

settings = Settings()
```

### `backend/rag/utils.py`
```python
import io, re, os, pathlib, tempfile
import fitz  # PyMuPDF
from bs4 import BeautifulSoup
import html2text as h2t
from readability import Document
from markdown_it import MarkdownIt

SUPPORTED_EXTS = {".pdf", ".md", ".markdown", ".html", ".htm", ".txt"}

md = MarkdownIt()


def clean_text(text: str) -> str:
    text = re.sub(r"\s+", " ", text)
    return text.strip()


def read_pdf(file_bytes: bytes) -> str:
    text_parts = []
    with fitz.open(stream=file_bytes, filetype="pdf") as doc:
        for page in doc:
            text_parts.append(page.get_text("text"))
    return clean_text("\n".join(text_parts))


def read_markdown(s: str) -> str:
    # Normalize MD → plain text via MarkdownIt tokens
    html = md.render(s)
    return read_html(html)


def read_html(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    # Drop scripts/styles
    for tag in soup(["script", "style", "noscript"]):
        tag.decompose()
    txt = soup.get_text(separator=" ")
    return clean_text(txt)


def read_url(url: str) -> str:
    import requests
    resp = requests.get(url, timeout=20)
    resp.raise_for_status()
    doc = Document(resp.text)
    html = doc.summary(html_partial=True)
    return read_html(html)


def extract_text_from_upload(filename: str, data: bytes) -> str:
    ext = pathlib.Path(filename).suffix.lower()
    if ext == ".pdf":
        return read_pdf(data)
    if ext in {".md", ".markdown"}:
        return read_markdown(data.decode("utf-8", errors="ignore"))
    if ext in {".html", ".htm"}:
        return read_html(data.decode("utf-8", errors="ignore"))
    if ext == ".txt":
        return clean_text(data.decode("utf-8", errors="ignore"))
    raise ValueError(f"Unsupported file type: {ext}")
```

### `backend/rag/chunker.py`
```python
from typing import List, Dict

# Simple token-ish chunking by word count for portability

def chunk_text(text: str, chunk_size: int = 450, overlap: int = 50) -> List[Dict]:
    words = text.split()
    chunks = []
    i = 0
    while i < len(words):
        start = max(i - overlap, 0)
        end = min(i + chunk_size, len(words))
        chunk_words = words[i:end]
        chunk_text = " ".join(chunk_words)
        chunks.append({
            "content": chunk_text,
            "start_word": i,
            "end_word": end,
        })
        i += chunk_size
    return chunks
```

### `backend/rag/llm.py`
```python
from .settings import settings

# Embeddings

def get_embedder():
    if settings.embeddings_provider == "openai":
        from openai import OpenAI
        client = OpenAI(api_key=settings.openai_api_key)
        model = settings.embeddings_model
        def embed(texts):
            resp = client.embeddings.create(model=model, input=texts)
            return [d.embedding for d in resp.data]
        return embed
    elif settings.embeddings_provider == "sentence-transformers":
        from sentence_transformers import SentenceTransformer
        model = SentenceTransformer(settings.embeddings_model)
        def embed(texts):
            return model.encode(texts, normalize_embeddings=True).tolist()
        return embed
    elif settings.embeddings_provider == "cohere":
        import cohere
        co = cohere.Client(settings.cohere_api_key)
        model = settings.embeddings_model
        def embed(texts):
            resp = co.embed(texts=texts, model=model)
            return resp.embeddings
        return embed
    else:
        raise ValueError("Unsupported embeddings provider")

# LLMs

def get_llm():
    if settings.llm_provider == "openai":
        from openai import OpenAI
        client = OpenAI(api_key=settings.openai_api_key)
        model = settings.llm_model
        def complete(system_prompt, user_prompt):
            resp = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=0.2,
            )
            return resp.choices[0].message.content
        return complete
    elif settings.llm_provider == "ollama":
        import requests
        def complete(system_prompt, user_prompt):
            payload = {
                "model": settings.llm_model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                "options": {"temperature": 0.2}
            }
            r = requests.post("http://localhost:11434/api/chat", json=payload, timeout=120)
            r.raise_for_status()
            return r.json()["message"]["content"] if "message" in r.json() else r.json().get("response", "")
        return complete
    else:
        raise ValueError("Unsupported LLM provider")
```

### `backend/rag/retriever.py`
```python
import os
from typing import List, Dict
import numpy as np
from .settings import settings
from .llm import get_embedder

class VectorStore:
    def add_texts(self, texts: List[str], metadatas: List[Dict]): ...
    def similarity_search(self, query: str, k: int = 4) -> List[Dict]: ...

class ChromaStore(VectorStore):
    def __init__(self, collection_name: str = "docs"):
        import chromadb
        from chromadb.utils import embedding_functions
        self._client = chromadb.PersistentClient(path=settings.persist_dir)
        self._embed = get_embedder()
        self._col = self._client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}
        )

    def add_texts(self, texts: List[str], metadatas: List[Dict]):
        ids = [f"id_{i}_{len(texts[i])}" for i in range(len(texts))]
        embeddings = self._embed(texts)
        self._col.add(documents=texts, metadatas=metadatas, ids=ids, embeddings=embeddings)

    def similarity_search(self, query: str, k: int = 4) -> List[Dict]:
        emb = self._embed([query])[0]
        res = self._col.query(query_embeddings=[emb], n_results=k, include=["metadatas", "documents", "distances"])
        docs = []
        for i in range(len(res["documents"][0])):
            docs.append({
                "content": res["documents"][0][i],
                "metadata": res["metadatas"][0][i],
                "score": 1 - res["distances"][0][i],  # cosine → similarity
            })
        return docs

class FaissStore(VectorStore):
    def __init__(self):
        import faiss
        self._embed = get_embedder()
        self._dim = len(self._embed(["dim"][0:1])[0])
        self._index = faiss.IndexFlatIP(self._dim)
        self._metas: List[Dict] = []
        self._texts: List[str] = []

    def add_texts(self, texts: List[str], metadatas: List[Dict]):
        embs = np.array(self._embed(texts), dtype="float32")
        # Normalize for cosine similarity with inner product
        faiss = __import__("faiss")
        faiss.normalize_L2(embs)
        self._index.add(embs)
        self._texts.extend(texts)
        self._metas.extend(metadatas)

    def similarity_search(self, query: str, k: int = 4) -> List[Dict]:
        import faiss
        q = np.array([self._embed([query])[0]], dtype="float32")
        faiss.normalize_L2(q)
        D, I = self._index.search(q, k)
        out = []
        for idx, score in zip(I[0], D[0]):
            if idx == -1: continue
            out.append({"content": self._texts[idx], "metadata": self._metas[idx], "score": float(score)})
        return out


def make_store() -> VectorStore:
    if settings.vector_db == "chroma":
        os.makedirs(settings.persist_dir, exist_ok=True)
        return ChromaStore()
    return FaissStore()
```

### `backend/rag/ingest.py`
```python
from typing import List, Dict
from .utils import extract_text_from_upload, read_url
from .chunker import chunk_text
from .retriever import make_store

store = make_store()


def ingest_file(filename: str, data: bytes, source_name: str | None = None, chunk_size: int = 450) -> int:
    text = extract_text_from_upload(filename, data)
    chunks = chunk_text(text, chunk_size=chunk_size)
    store.add_texts([c["content"] for c in chunks], [
        {"source": source_name or filename, "type": "file", "start_word": c["start_word"], "end_word": c["end_word"]}
        for c in chunks
    ])
    return len(chunks)


def ingest_url(url: str, source_name: str | None = None, chunk_size: int = 450) -> int:
    text = read_url(url)
    chunks = chunk_text(text, chunk_size=chunk_size)
    store.add_texts([c["content"] for c in chunks], [
        {"source": source_name or url, "type": "url", "start_word": c["start_word"], "end_word": c["end_word"]}
        for c in chunks
    ])
    return len(chunks)
```

### `backend/models.py`
```python
from pydantic import BaseModel
from typing import List, Dict

class QueryRequest(BaseModel):
    question: str
    k: int = 4

class SourceChunk(BaseModel):
    content: str
    metadata: Dict
    score: float

class QueryResponse(BaseModel):
    answer: str
    sources: List[SourceChunk]
```

### `backend/main.py`
```python
from fastapi import FastAPI, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from typing import List
from rag.settings import settings
from rag.ingest import ingest_file, ingest_url, store
from rag.llm import get_llm
from rag.retriever import make_store
from models import QueryRequest, QueryResponse, SourceChunk

app = FastAPI(title="RAG Backend")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

llm = get_llm()

SYSTEM_PROMPT = (
    "You are a helpful assistant that answers strictly based on the provided context. "
    "If the answer is not in the context, say you don't know. When possible, cite the source names."
)

@app.get("/health")
async def health():
    return {"status": "ok"}

@app.post("/ingest/file")
async def ingest_file_ep(file: UploadFile = File(...), source_name: str = Form(default=None)):
    data = await file.read()
    n = ingest_file(file.filename, data, source_name)
    return {"chunks": n}

@app.post("/ingest/url")
async def ingest_url_ep(url: str = Form(...), source_name: str = Form(default=None)):
    n = ingest_url(url, source_name)
    return {"chunks": n}

@app.post("/query", response_model=QueryResponse)
async def query(req: QueryRequest):
    docs = store.similarity_search(req.question, k=req.k)
    context = "\n\n".join([f"[Source: {d['metadata'].get('source','unknown')}]\n{d['content']}" for d in docs])
    prompt = (
        f"Context:\n{context}\n\nQuestion: {req.question}\n\n"
        "Answer with citations like [source]."
    )
    answer = llm(SYSTEM_PROMPT, prompt)
    sources = [SourceChunk(content=d["content"], metadata=d["metadata"], score=d.get("score", 0.0)) for d in docs]
    return QueryResponse(answer=answer, sources=sources)
```

### `backend/README.md`
```
# RAG Backend

## Setup
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
cp .env.example .env  # fill keys

## Run
uvicorn main:app --reload --host 0.0.0.0 --port 8000

## Try
curl -F "file=@/path/to/file.pdf" http://localhost:8000/ingest/file
curl -F "url=https://en.wikipedia.org/wiki/Machine_learning" http://localhost:8000/ingest/url
curl -H 'Content-Type: application/json' \
     -d '{"question":"What is supervised learning?","k":4}' \
     http://localhost:8000/query
```

---

## 🖥️ 2) Frontend (React + Vite + Tailwind)

**Structure**
```
frontend/
  ├─ index.html
  ├─ package.json
  ├─ vite.config.ts
  ├─ tailwind.config.js
  ├─ postcss.config.js
  └─ src/
     ├─ main.tsx
     ├─ App.tsx
     ├─ components/
     │   ├─ Chat.tsx
     │   ├─ FileUpload.tsx
     │   └─ SourceCard.tsx
     └─ types.ts
```

### `frontend/package.json`
```json
{
  "name": "rag-frontend",
  "version": "0.1.0",
  "private": true,
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "vite build",
    "preview": "vite preview"
  },
  "dependencies": {
    "react": "^18.3.1",
    "react-dom": "^18.3.1"
  },
  "devDependencies": {
    "@types/react": "^18.3.3",
    "@types/react-dom": "^18.3.0",
    "autoprefixer": "^10.4.20",
    "postcss": "^8.4.41",
    "tailwindcss": "^3.4.10",
    "typescript": "^5.5.4",
    "vite": "^5.4.2"
  }
}
```

### `frontend/tailwind.config.js`
```js
/** @type {import('tailwindcss').Config} */
export default {
  content: ["./index.html", "./src/**/*.{ts,tsx}"],
  theme: { extend: {} },
  plugins: [],
};
```

### `frontend/postcss.config.js`
```js
export default {
  plugins: {
    tailwindcss: {},
    autoprefixer: {},
  },
};
```

### `frontend/vite.config.ts`
```ts
import { defineConfig } from 'vite'
import react from '@vitejs/plugin-react'

export default defineConfig({
  plugins: [react()],
  server: { port: 5173 },
})
```

### `frontend/index.html`
```html
<!doctype html>
<html>
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>RAG Q&A</title>
    <link rel="icon" href="data:," />
  </head>
  <body class="bg-gray-50">
    <div id="root"></div>
    <script type="module" src="/src/main.tsx"></script>
  </body>
</html>
```

### `frontend/src/main.tsx`
```tsx
import React from 'react'
import { createRoot } from 'react-dom/client'
import App from './App'
import './index.css'

createRoot(document.getElementById('root')!).render(
  <React.StrictMode>
    <App />
  </React.StrictMode>
)
```

### `frontend/src/index.css`
```css
@tailwind base;
@tailwind components;
@tailwind utilities;
```

### `frontend/src/types.ts`
```ts
export type SourceChunk = {
  content: string;
  metadata: Record<string, any>;
  score: number;
};

export type QueryResponse = {
  answer: string;
  sources: SourceChunk[];
};
```

### `frontend/src/components/SourceCard.tsx`
```tsx
import React from 'react'
import { SourceChunk } from '../types'

export default function SourceCard({ s }: { s: SourceChunk }) {
  return (
    <div className="rounded-2xl border bg-white p-3 shadow-sm">
      <div className="text-sm text-gray-700 whitespace-pre-wrap">{s.content.slice(0, 600)}{s.content.length>600?'…':''}</div>
      <div className="mt-2 flex items-center justify-between text-xs text-gray-500">
        <span className="font-medium">{s.metadata?.source || 'unknown'}</span>
        <span>score: {s.score.toFixed(3)}</span>
      </div>
    </div>
  )
}
```

### `frontend/src/components/FileUpload.tsx`
```tsx
import React, { useState } from 'react'

export default function FileUpload({ backendUrl }: { backendUrl: string }) {
  const [busy, setBusy] = useState(false)
  const [url, setUrl] = useState('')

  const onFileChange = async (e: React.ChangeEvent<HTMLInputElement>) => {
    const f = e.target.files?.[0]
    if (!f) return
    setBusy(true)
    const form = new FormData()
    form.append('file', f)
    const res = await fetch(`${backendUrl}/ingest/file`, { method: 'POST', body: form })
    setBusy(false)
    alert(`Ingested: ${(await res.json()).chunks} chunks`)
  }

  const onIngestUrl = async () => {
    if (!url) return
    setBusy(true)
    const form = new FormData()
    form.append('url', url)
    const res = await fetch(`${backendUrl}/ingest/url`, { method: 'POST', body: form })
    setBusy(false)
    alert(`Ingested: ${(await res.json()).chunks} chunks`)
  }

  return (
    <div className="flex gap-2 items-center">
      <input type="file" accept=".pdf,.md,.markdown,.html,.htm,.txt" onChange={onFileChange} disabled={busy}
             className="block w-full text-sm text-gray-900 border border-gray-300 rounded-lg cursor-pointer bg-gray-50" />
      <input value={url} onChange={e=>setUrl(e.target.value)} placeholder="https://example.com/page" className="px-3 py-2 rounded-lg border w-96" />
      <button onClick={onIngestUrl} disabled={busy} className="px-4 py-2 rounded-xl bg-black text-white">Add URL</button>
    </div>
  )
}
```

### `frontend/src/components/Chat.tsx`
```tsx
import React, { useState } from 'react'
import { QueryResponse } from '../types'
import SourceCard from './SourceCard'

export default function Chat({ backendUrl }: { backendUrl: string }) {
  const [q, setQ] = useState('')
  const [ans, setAns] = useState<QueryResponse | null>(null)
  const [busy, setBusy] = useState(false)

  const ask = async () => {
    if (!q.trim()) return
    setBusy(true)
    const res = await fetch(`${backendUrl}/query`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ question: q, k: 4 }),
    })
    const data = await res.json()
    setAns(data)
    setBusy(false)
  }

  return (
    <div className="space-y-4">
      <div className="flex gap-2">
        <input value={q} onChange={e=>setQ(e.target.value)} placeholder="Ask anything about your docs…" className="px-4 py-3 rounded-2xl border w-full" />
        <button onClick={ask} disabled={busy} className="px-5 py-3 rounded-2xl bg-black text-white">{busy? 'Thinking…' : 'Ask'}</button>
      </div>

      {ans && (
        <div className="space-y-3">
          <div className="rounded-2xl bg-white p-4 shadow-sm border">
            <div className="font-semibold mb-1">Answer</div>
            <div className="whitespace-pre-wrap text-gray-800">{ans.answer}</div>
          </div>
          <div className="grid md:grid-cols-2 gap-3">
            {ans.sources.map((s,i)=>(<SourceCard key={i} s={s} />))}
          </div>
        </div>
      )}
    </div>
  )
}
```

### `frontend/src/App.tsx`
```tsx
import React from 'react'
import FileUpload from './components/FileUpload'
import Chat from './components/Chat'

const backendUrl = import.meta.env.VITE_BACKEND_URL || 'http://localhost:8000'

export default function App() {
  return (
    <div className="max-w-5xl mx-auto p-6">
      <header className="mb-6">
        <h1 className="text-2xl font-bold">RAG Q&A</h1>
        <p className="text-gray-600">Upload PDFs/Markdown/HTML or add URLs. Ask questions and get cited answers.</p>
      </header>
      <main className="space-y-6">
        <FileUpload backendUrl={backendUrl} />
        <Chat backendUrl={backendUrl} />
      </main>
      <footer className="text-xs text-gray-500 mt-10">Embeddings/LLM configurable via backend .env</footer>
    </div>
  )
}
```

---

## 🐳 3) Docker & Compose

### `docker-compose.yml`
```yaml
version: "3.8"
services:
  backend:
    build: ./backend
    env_file:
      - ./backend/.env
    volumes:
      - ./backend/storage:/app/storage
    ports:
      - "8000:8000"

  frontend:
    build: ./frontend
    environment:
      - VITE_BACKEND_URL=http://localhost:8000
    ports:
      - "5173:5173"
    depends_on:
      - backend
```

### `backend/Dockerfile`
```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt
COPY . ./
EXPOSE 8000
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### `frontend/Dockerfile`
```dockerfile
FROM node:20-alpine
WORKDIR /app
COPY package.json package-lock.json* ./
RUN npm ci || npm i
COPY . .
EXPOSE 5173
CMD ["npm", "run", "dev", "--", "--host"]
```

---

## 🚀 4) Local Dev Quickstart

```bash
# 1) Clone and enter
# git clone <this-repo>
cd backend
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
cp .env.example .env
# set OPENAI_API_KEY (or switch to sentence-transformers + local model)

# 2) Run backend
uvicorn main:app --reload

# 3) Run frontend
cd ../frontend
npm i
VITE_BACKEND_URL=http://localhost:8000 npm run dev
```

Open http://localhost:5173 and start uploading/asking.

---

## 🧪 5) Smoke Test Scenario
1) Ingest a PDF (`/ingest/file`) and a Wikipedia URL (`/ingest/url`).
2) Ask: *"What is the difference between supervised and unsupervised learning?"*
3) Expect: a short comparison + `[source]`-style mentions and source cards under the answer.

---

## ✨ 6) Optional Enhancements (pointers inside the current code)
- **Citations with spans**: store chunk offsets and highlight matched spans in `SourceCard`.
- **Multi-doc namespaces**: add `namespace` in metadata and route queries per-namespace.
- **Auth**: add API keys/JWT to FastAPI.
- **Re-ranking**: add a cross-encoder re-ranker (e.g., `cross-encoder/ms-marco-MiniLM-L-6-v2`).
- **Streaming answers**: switch to server-sent events and stream tokens.
- **Chat history**: maintain conversation context in frontend state and include recent Q/A as additional context.
- **Notion export ingestion**: accept `.zip` of MD/HTML, unzip server-side, batch-ingest.

---

## 🛡️ 7) Production Tips
- Use **Qdrant/Pinecone/Weaviate** when scaling beyond a single machine.
- Add **rate limiting** and **payload size limits**.
- Persist a **document table** with checksums and per-chunk IDs for deletions/updates.
- Log **retrieval metrics** (hit rate, MRR) and add evaluation sets.

---

## 📚 8) Minimal API Reference
- `POST /ingest/file`: multipart `file`, optional `source_name`
- `POST /ingest/url`: form `url`, optional `source_name`
- `POST /query`: `{ question: string, k?: number }` → `{ answer, sources[] }`
- `GET /health`: service status

---

## 🧩 9) Notes on Models
- **OpenAI embeddings**: `text-embedding-3-small` (fast/cheap) or `-large` (higher quality)
- **Local embeddings**: `sentence-transformers` (e.g., `all-MiniLM-L6-v2`)
- **LLMs**: OpenAI (`gpt-4o-mini`), or **Ollama** locally (`llama3`, `mistral`, etc.).

---

Happy building! 🎯
